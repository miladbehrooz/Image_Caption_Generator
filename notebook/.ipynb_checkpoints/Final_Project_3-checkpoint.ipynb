{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51358f7a-ecd1-4293-92e5-0b08411c2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fefc14fa-13ce-4214-a39c-2742428f8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.pkl', 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0214141-54aa-4b90-b6ed-a54f9b58d8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6e27f-4ceb-423e-b033-943a68113032",
   "metadata": {},
   "source": [
    "### Text Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f52aeb7d-9246-44c3-acb8-f24e71cdac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(filepath):\n",
    "    \"\"\"\n",
    "    load text file from given filepath\n",
    "    \"\"\"\n",
    "    with open(filepath,'r') as file:\n",
    "        text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f121ca21-390f-4593-8bad-2d2c2ca39a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions(text):\n",
    "    \"\"\"\n",
    "    extract the image id and related image captions from text file and \n",
    "    retuern them in a dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    captions = text.split('\\n')\n",
    "    descriptions = {}\n",
    "    for caption in captions[:-1]:\n",
    "        image_id , image_des = caption.split('\\t')\n",
    "        image_id = image_id.split('.')[0]\n",
    "\n",
    "        if image_id not in descriptions:\n",
    "            descriptions[image_id] = [image_des] \n",
    "        else:\n",
    "            descriptions[image_id].append(image_des)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc37d9f6-7c6a-4786-b18a-058f74fa4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_captions(descriptions, N=5):\n",
    "    \"\"\"\n",
    "    return image_id and given number of related captions as a dictionary\n",
    "    \"\"\"\n",
    "    descriptions_reduced = dict()\n",
    "    for image_id, captions in descriptions.items():\n",
    "        descriptions_reduced[image_id] = captions[0:N]\n",
    "    return descriptions_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "925117e9-2f13-468c-bc42-851d3f74d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_captions(descriptions):\n",
    "    \"\"\"\n",
    "    clean the captions text. This function converts all words to lowercase, romves punctuation,\n",
    "    remove all words which are less than one character and removes words which include of numbers\n",
    "    \"\"\"\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    for image_id,image_captions in descriptions.items():\n",
    "        for i,image_caption in enumerate(image_captions):\n",
    "            image_caption.replace(\"-\",\" \")\n",
    "            # split the captions into separate words\n",
    "            descp = image_caption.split()\n",
    "            #convert uppercase to lowercase\n",
    "            descp = [word.lower() for word in descp]\n",
    "            #print(descp)\n",
    "            #remove punctuations\n",
    "            descp = [word.translate(table) for word in descp]\n",
    "            #remove words which are less than one character e.g. 's and a\n",
    "            descp = [word for word in descp if(len(word)>1)]\n",
    "            #remove words which include of numbers\n",
    "            descp = [word for word in descp if(word.isalpha())]\n",
    "            image_caption = 'startseq ' + ' '.join(descp) +  ' endseq'\n",
    "            descriptions[image_id][i]= image_caption\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6b0a25-731b-4d91-a312-66346a84185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_celaned_captions(captions, filename):\n",
    "    \"\"\"\n",
    "    save cleaned captions in the given file\n",
    "    \"\"\"\n",
    "    lines = list()\n",
    "    for image_id, captions_list in captions.items():\n",
    "        for caption in captions_list:\n",
    "            lines.append(image_id + '\\t' + caption)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c643754-5d36-4132-b18f-71db188f6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load token text file\n",
    "text_filename = '../data/Flickr8k_text/Flickr8k.token2.txt'\n",
    "text = load_text_file(text_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e086b00-f269-45de-bda7-5eb4565c9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the image id and related image captions\n",
    "descriptions = get_captions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72504c1e-5378-4fa7-b8f8-7802b65c9592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptions  = reduce_captions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07a8cb4e-52df-4d85-b7f6-9506ec53d4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a998e6a0-f7ca-4f5f-93f9-49458e31ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_captions = clean_captions(descriptions)\n",
    "save_celaned_captions(cleaned_captions, '../data/cleaned_captions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fb381-3592-42b8-a8f7-cc17da8b8146",
   "metadata": {},
   "source": [
    "### Text Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ee61bf2-c796-4f8e-81da-4d309b0a4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_collector(descriptions):\n",
    "    \"\"\"\n",
    "    collect all the image captions and return them in one list\n",
    "    \"\"\"\n",
    "    all_captions = []\n",
    "    for image_id in descriptions:\n",
    "        for image_caption in descriptions[image_id]:\n",
    "            all_captions.append(image_caption)\n",
    "    return all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f4c61b5-6580-4f1c-9600-f62548422b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(descriptions):\n",
    "    \"\"\"\n",
    "    fit a tokenizer to a given caption descriptions\n",
    "    \"\"\"\n",
    "    all_captions_list = caption_collector(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_captions_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27bfa826-106a-4b64-a754-167cd3ee30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(descriptions):\n",
    "    \"\"\"\n",
    "    return the maximum length of the caption in the description\n",
    "    \"\"\"\n",
    "    all_captions_list = caption_collector(descriptions)\n",
    "    return max(len(caption.split()) for caption in all_captions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "693e1542-bab6-4e8b-a0b2-a4006de956e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8766"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = build_tokenizer(cleaned_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ec7b0e7-dfa9-48f7-b4d0-b335c8db6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "133e526f-b75c-4ea4-ab63-299787d4f130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max_length(cleaned_captions)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5824c-4a94-4b42-ab1c-73bd23b377e1",
   "metadata": {},
   "source": [
    "### Train Validation Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b23ff6e-20e4-4296-b295-8ea1e0d4f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_split(descriptions, train_size = 0.60, validation_size = 0.20):\n",
    "    image_ids = list(descriptions.keys())\n",
    "    random.shuffle(image_ids)\n",
    "\n",
    "    train_split = int(len(image_ids) * train_size)\n",
    "    validation_split = int(len(image_ids) * (train_size + validation_size))\n",
    "\n",
    "    train = image_ids[:train_split]\n",
    "    validation = image_ids[train_split:validation_split]\n",
    "    test = image_ids[validation_split:]\n",
    "    return train,validation,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "031654cb-20be-4be1-b6fa-a6bc879daaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id_train,image_id_validation,image_id_test = train_validation_test_split(cleaned_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4e9af32-f5d4-4bce-98e5-55256c884ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4854, 1618, 1619)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_id_train),len(image_id_validation), len(image_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61becde2-13a3-46aa-b355-9ad3fbae3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image_ids, cleaned_captions, features, max_len, tokenizer, vocab_size):\n",
    "    \"\"\"\n",
    "    ??\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for image_id in image_ids:\n",
    "        captions = cleaned_captions[image_id]\n",
    "        for caption in captions: \n",
    "            text = tokenizer.texts_to_sequences([caption])[0]\n",
    "            for i in range(1,len(text)):\n",
    "                in_text, out_text = text[:i],text[i]\n",
    "                in_text = pad_sequences([in_text], maxlen=max_len)[0]\n",
    "                out_text = to_categorical([out_text], num_classes=vocab_size)[0]\n",
    "                X1.append(features[image_id][0])\n",
    "                X2.append(in_text)\n",
    "                y.append(out_text)\n",
    "    return np.array(X1),np.array(X2),np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b101b-1f4e-4e38-bb64-008ecd169eb6",
   "metadata": {},
   "source": [
    "### Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "022a4ec1-cf91-4c2c-8bdb-1dc9c844b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, max_len):\n",
    "    # encoder model\n",
    "    # image features from CNN model squeezed from 4096 to 256 nodes\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1) # change to 0.3 or 0.2\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_len,))\n",
    "    se1 = Embedding(vocab_size, 64, mask_zero=True)(inputs2) # change 256 to 64\n",
    "    se2 = Dropout(0.5)(se1) # change to 0.3 or 0.2\n",
    "    se3 = LSTM(256)(se2)\n",
    "    \n",
    "    # decoder model\n",
    "    # merge both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2) # test sigmoid\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9534b58c-a7b3-4db4-b905-576ab81214d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 34, 64)       561024      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 34, 64)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          328704      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8766)         2252862     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,257,214\n",
      "Trainable params: 4,257,214\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = build_model(vocab_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2433909-b727-4c0a-8d29-11e600ef21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X2_train, y_train = preprocessing(image_id_train, cleaned_captions,\n",
    "                                            features, max_len, tokenizer, vocab_size)\n",
    "X1_val, X2_val, y_val = preprocessing(image_id_validation, cleaned_captions,\n",
    "                                      features, max_len, tokenizer, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b330c690-561f-414b-a662-c3794be4bd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((248163, 4096), (248163, 34), (248163, 8766), (82530, 4096))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train.shape, X2_train.shape, y_train.shape,X1_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87c4a995-140a-42f5-bc7e-e4b5df1428e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "filepath = './models_5_4800_64/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce10c425-783f-4c8b-a7a2-661a03e6afe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "826/826 [==============================] - 516s 620ms/step - loss: 4.8211 - val_loss: 4.2848\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.28484, saving model to ./models_4_4800_64/model-ep001-loss4.821-val_loss4.285.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "826/826 [==============================] - 761s 921ms/step - loss: 3.8248 - val_loss: 4.0453\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.28484 to 4.04534, saving model to ./models_4_4800_64/model-ep002-loss3.825-val_loss4.045.h5\n",
      "Epoch 3/5\n",
      "826/826 [==============================] - 902s 1s/step - loss: 3.4622 - val_loss: 4.0193\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.04534 to 4.01929, saving model to ./models_4_4800_64/model-ep003-loss3.462-val_loss4.019.h5\n",
      "Epoch 4/5\n",
      "826/826 [==============================] - 1341s 2s/step - loss: 3.2258 - val_loss: 4.0382\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.01929\n",
      "Epoch 5/5\n",
      "826/826 [==============================] - 1359s 2s/step - loss: 3.0407 - val_loss: 4.0999\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.01929\n"
     ]
    }
   ],
   "source": [
    "# clear Keras backend session\n",
    "K.clear_session()\n",
    "hist = model.fit([X1_train, X2_train], y_train, \n",
    "                  epochs=5, verbose=1, batch_size=300,\n",
    "                  validation_data=([X1_val, X2_val], y_val),callbacks=[checkpoint])#, 20 epoch\n",
    "                  #callbacks = [early_stop]) # change the validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef1c560c-bd21-4667-9974-b3dde72741c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for label in [\"loss\",\"val_loss\"]:\n",
    "    plt.plot(hist.history[label],label=label)\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b1876-bf19-41d8-b164-c8106ac6033a",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83d13756-e310-445a-94a3-a6ef2ae2c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models_5_4800_64/model-ep003-loss3.462-val_loss4.019.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7be9c86d-a328-4202-8f64-72cfb8b52986",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = dict([(index,word) for word, index in tokenizer.word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc77a955-9102-47ec-94fc-4d04290dad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image,tokenizer,max_len): # image_feature instead of image\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_len):\n",
    "        seq = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        seq = pad_sequences([seq],max_len)\n",
    "        y_pred = model.predict([image,seq],verbose = 0)\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        new_word = index_to_word[y_pred]\n",
    "        if new_word is None:\n",
    "            break\n",
    "        in_text += \" \" + new_word\n",
    "        if new_word == \"endseq\":\n",
    "            break\n",
    "        \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a2eb4de-97e8-4baa-b616-06b39d59cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_id):\n",
    "    actual = []\n",
    "    bleu = 0    \n",
    "    image_path = '../data/Flicker8k_Dataset/' + image_id + '.jpg'\n",
    "    img = load_img(image_path,target_size=(224, 224))\n",
    "    captions = cleaned_captions[image_id]\n",
    "    y_pred = predict_caption(model, features[image_id],tokenizer,max_len)\n",
    "    print('----Actual----')\n",
    "    for caption in captions:\n",
    "        caption= caption.split()[1:-1]\n",
    "        actual.append(caption)\n",
    "        caption =  \" \".join(caption)\n",
    "        print(caption)\n",
    "    print(actual)\n",
    "    bleu = corpus_bleu([actual],y_pred)\n",
    "    print('\\n')\n",
    "    \n",
    "    print('---Predicted----')\n",
    "    y_pred = y_pred.split()[1:-1]\n",
    "    y_pred =  \" \".join(y_pred)\n",
    "    print(y_pred)\n",
    "    bleu = bleu / len(captions)\n",
    "    print('\\n')\n",
    "    print(f'BLEU= {bleu}')\n",
    "    plt.imshow(img)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2219310-2a08-4771-8b35-5543c5619f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Actual----\n",
      "photo of smiling young woman in front of tables on the street\n",
      "woman in jacket is posing for camera smiling in mostly empty restaurant\n",
      "young girl standing outside restaurant at night\n",
      "young woman wearing turtle neck and black jacket is smiling for picture\n",
      "the little girl is wearing black sweater and standing in front of restaurant\n",
      "[['photo', 'of', 'smiling', 'young', 'woman', 'in', 'front', 'of', 'tables', 'on', 'the', 'street'], ['woman', 'in', 'jacket', 'is', 'posing', 'for', 'camera', 'smiling', 'in', 'mostly', 'empty', 'restaurant'], ['young', 'girl', 'standing', 'outside', 'restaurant', 'at', 'night'], ['young', 'woman', 'wearing', 'turtle', 'neck', 'and', 'black', 'jacket', 'is', 'smiling', 'for', 'picture'], ['the', 'little', 'girl', 'is', 'wearing', 'black', 'sweater', 'and', 'standing', 'in', 'front', 'of', 'restaurant']]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The number of hypotheses and their reference(s) should be the same ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-fbf621ddbe3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m63\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-259b34d9fa04>\u001b[0m in \u001b[0;36mgenerate_caption\u001b[0;34m(image_id)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning/lib/python3.6/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     assert len(list_of_references) == len(hypotheses), (\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;34m\"The number of hypotheses and their reference(s) should be the \"\u001b[0m \u001b[0;34m\"same \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The number of hypotheses and their reference(s) should be the same "
     ]
    }
   ],
   "source": [
    "generate_caption(image_id_train[63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e0073db-eeda-4d32-a72c-bf3665eecc79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.272561325903966e-155,\n",
       " 0,\n",
       " 5.330020010319017e-232,\n",
       " 2.3298299857550653e-155,\n",
       " 8.396161215621529e-232,\n",
       " 0,\n",
       " 1.1862177682648818e-231,\n",
       " 7.296382734947757e-232,\n",
       " 4.65988169467104e-155,\n",
       " 6.416038883891965e-155,\n",
       " 3.6697394677640926e-155,\n",
       " 0,\n",
       " 3.648956622645728e-232,\n",
       " 5.395774370246974e-78,\n",
       " 3.571883312829026e-232,\n",
       " 1.1721660986694186e-78,\n",
       " 6.787872662925505e-155,\n",
       " 3.8476696299631795e-155,\n",
       " 1.0003688322288243e-231,\n",
       " 3.2465087977690726e-78,\n",
       " 5.705336763108155e-78,\n",
       " 2.965165608665095e-78,\n",
       " 0,\n",
       " 7.657404561915943e-155,\n",
       " 4.481994719908145e-232,\n",
       " 6.520501036082648e-232,\n",
       " 6.7393716283177006e-155,\n",
       " 1.384292958842266e-231,\n",
       " 9.609501042934093e-156,\n",
       " 0.2493651438887133,\n",
       " 1.0003688322288243e-231,\n",
       " 5.791739854583281e-155,\n",
       " 1.331960397810445e-231,\n",
       " 3.935039685526131e-155,\n",
       " 1.0832677820940877e-231,\n",
       " 1.0244914152188952e-231,\n",
       " 6.010455532087301e-155,\n",
       " 0,\n",
       " 3.571883312829026e-232,\n",
       " 1.0518351895246305e-231,\n",
       " 4.27242167093289e-155,\n",
       " 8.422437779564611e-232,\n",
       " 4.35483727327488e-155,\n",
       " 1.0518351895246305e-231,\n",
       " 0,\n",
       " 7.100514228897259e-155,\n",
       " 1.2387197655613557e-231,\n",
       " 5.058927350602078e-232,\n",
       " 0,\n",
       " 1.0658543616184898e-231,\n",
       " 1.331960397810445e-231,\n",
       " 6.510101141652543e-232,\n",
       " 7.060301868108111e-232,\n",
       " 6.968148412761692e-155,\n",
       " 9.709385502639237e-232,\n",
       " 0.2984745896009823,\n",
       " 0,\n",
       " 0.3655552228545123,\n",
       " 1.2183324802375697e-231,\n",
       " 4.7706101635859015e-232,\n",
       " 8.612150057732663e-155,\n",
       " 0,\n",
       " 1.0518351895246305e-231,\n",
       " 0.32260135189272865,\n",
       " 1.0518351895246305e-231,\n",
       " 0,\n",
       " 1.2882297539194154e-231,\n",
       " 1.0749662678465409e-231,\n",
       " 0.5578002860768766,\n",
       " 2.9251770022770788e-232,\n",
       " 0,\n",
       " 9.269981669466712e-232,\n",
       " 8.853864984883467e-232,\n",
       " 1.1200407237786664e-231,\n",
       " 1.1640469867513693e-231,\n",
       " 3.645525559050358e-155,\n",
       " 7.107197028258987e-232,\n",
       " 1.331960397810445e-231,\n",
       " 0,\n",
       " 1.126199256928023e-231,\n",
       " 8.34076112986429e-232,\n",
       " 1.2183324802375697e-231,\n",
       " 2.6616657200018397e-78,\n",
       " 1.1640469867513693e-231,\n",
       " 1.3483065280626046e-231,\n",
       " 5.330020010319017e-232,\n",
       " 3.271092989210003e-78,\n",
       " 7.601159375410181e-232,\n",
       " 1.2508498911928379e-231,\n",
       " 1.1640469867513693e-231,\n",
       " 7.711523862191631e-155,\n",
       " 2.6044836408156456e-78,\n",
       " 4.658566834617536e-155,\n",
       " 3.917372151449784e-155,\n",
       " 7.521821744402224e-232,\n",
       " 4.282289549833951e-232,\n",
       " 6.223629500679345e-155,\n",
       " 0.39011264866539486,\n",
       " 1.1640469867513693e-231,\n",
       " 9.037968939610782e-232,\n",
       " 6.8745561108800095e-155,\n",
       " 4.987693055201041e-155,\n",
       " 9.013778876140909e-155,\n",
       " 9.269981669466712e-232,\n",
       " 4.1927887725759647e-78,\n",
       " 1.1200407237786664e-231,\n",
       " 7.600394483915427e-155,\n",
       " 4.753148692240233e-232,\n",
       " 3.690840039559348e-232,\n",
       " 6.744160953836975e-232,\n",
       " 3.891524296988902e-155,\n",
       " 1.171778691554733e-231,\n",
       " 0,\n",
       " 6.480360213743594e-155,\n",
       " 0,\n",
       " 6.968148412761692e-155,\n",
       " 5.233427736988301e-155,\n",
       " 4.2338646970116024e-78,\n",
       " 1.0009379942300742e-231,\n",
       " 0,\n",
       " 1.0009379942300742e-231,\n",
       " 1.2508498911928379e-231,\n",
       " 6.373704167435469e-155,\n",
       " 1.2387197655613557e-231,\n",
       " 1.2183324802375697e-231,\n",
       " 6.416038883891965e-155,\n",
       " 8.416851712392762e-232,\n",
       " 6.510101141652543e-232,\n",
       " 9.853445011990208e-232,\n",
       " 9.418382295637229e-232,\n",
       " 1.1368587676511996e-231,\n",
       " 1.0244914152188952e-231,\n",
       " 1.2183324802375697e-231,\n",
       " 1.1896457329133973e-231,\n",
       " 6.061838450024688e-155,\n",
       " 2.509958499009158e-78,\n",
       " 1.0016022933125248e-231,\n",
       " 1.384292958842266e-231,\n",
       " 4.828970785282294e-155,\n",
       " 9.039352811507815e-232,\n",
       " 0,\n",
       " 0,\n",
       " 3.0858707633577667e-155,\n",
       " 5.722633035689358e-155,\n",
       " 1.331960397810445e-231,\n",
       " 5.203538639359665e-78,\n",
       " 6.034940380417626e-232,\n",
       " 1.2508498911928379e-231,\n",
       " 4.721756180471194e-155,\n",
       " 1.0518351895246305e-231,\n",
       " 0,\n",
       " 4.658566834617536e-155,\n",
       " 7.176381577237209e-155,\n",
       " 9.853445011990208e-232,\n",
       " 8.324264127738903e-232,\n",
       " 2.131874265859462e-155,\n",
       " 0,\n",
       " 1.171778691554733e-231,\n",
       " 0,\n",
       " 1.1200407237786664e-231,\n",
       " 0,\n",
       " 9.412234823955334e-232,\n",
       " 0.26708679784499234,\n",
       " 0,\n",
       " 5.6228231334895985e-155,\n",
       " 0,\n",
       " 3.4709523904299764e-155,\n",
       " 0,\n",
       " 4.310732902257592e-232,\n",
       " 1.1640469867513693e-231,\n",
       " 0,\n",
       " 1.0518351895246305e-231,\n",
       " 4.539324034745998e-155,\n",
       " 1.331960397810445e-231,\n",
       " 4.900008468089985e-232,\n",
       " 1.2508498911928379e-231,\n",
       " 0,\n",
       " 6.030654703641957e-155,\n",
       " 1.331960397810445e-231,\n",
       " 6.016112399618114e-232,\n",
       " 9.032704947274394e-155,\n",
       " 1.2886556614526506e-78,\n",
       " 0,\n",
       " 9.918892480173173e-232,\n",
       " 6.016112399618114e-232,\n",
       " 1.1862177682648818e-231,\n",
       " 9.291879812217675e-232,\n",
       " 5.722633035689358e-155,\n",
       " 9.55507953056397e-233,\n",
       " 1.3483065280626046e-231,\n",
       " 0,\n",
       " 0,\n",
       " 7.176381577237209e-155,\n",
       " 7.58961907326157e-232,\n",
       " 7.521821744402224e-232,\n",
       " 7.296382734947757e-232,\n",
       " 9.418382295637229e-232,\n",
       " 2.1761999267756727e-78,\n",
       " 1.0832677820940877e-231,\n",
       " 4.038234320052887e-232]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids = image_id_train\n",
    "count = 0\n",
    "bleus = []\n",
    "for image_id in image_ids:\n",
    "    count += 1\n",
    "    if count >200:\n",
    "        break\n",
    "    caption_true = cleaned_captions[image_id][1].split()[1:-1]\n",
    "    caption_pre = predict_caption(model, features[image_id],tokenizer,max_len)\n",
    "    caption_pre = caption_pre.split()[1:-1]\n",
    "    bleu = sentence_bleu([caption_true],caption_pre)\n",
    "    bleus.append(bleu)\n",
    "    #print(caption_true)\n",
    "    #print(caption_pre)\n",
    "bleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb4612fd-7952-408f-8d77-2b76c53fc7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 0.2493651438887133\n",
      "55 0.2984745896009823\n",
      "57 0.3655552228545123\n",
      "63 0.32260135189272865\n",
      "68 0.5578002860768766\n",
      "97 0.39011264866539486\n",
      "162 0.26708679784499234\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bleus)):\n",
    "    if bleus[i] > 0.05:\n",
    "        print(i,bleus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ce8fc-15c4-4406-9730-62be54d28068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
